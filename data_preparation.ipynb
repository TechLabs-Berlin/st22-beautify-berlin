{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the workspace\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "artworks = pd.read_table(\"beautified_boxes.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning and preparing the data\n",
    "\"\"\"\n",
    "Things still to do:\n",
    "label all of the remaining rows in the data set in the branch \"mock data\", then commit, push, merge into main and pull changes into this branch (\"data_preparation\"), because the data set is not up to date yet\n",
    "empty user ratings will be interpreted as nan by python -> handle this! I would say filling with average does not make sense, but a missing user rating definitely influenced my decision. Maybe fill with a new label, \n",
    "because I actually kinda handled it like that. If it had no label, chances were slimmer to get approval.\n",
    "Then have a look at the columns and so on. In this branch, the columns are a bit silly, but this may get resolved by pulling the real data set. Still, check it out when it comes to loading the data.\n",
    "Later on, when I use the data for the ML algorithm, I do not need all of the columns. For example, I do not actually need the ID, I think. I should think about dropping it here or I just keep it in the data set and \n",
    "later just select the needed columns.\n",
    "When just loading the data, I \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and test sets\n",
    "# remember that 650/1000 are labeled yet. Either label remaining ones or shorten the set\n",
    "# training data  is for training the algorithm \n",
    "# test set is for evaluating the algorithm\n",
    "# both must be strictly separated\n",
    "\n",
    "# features are type, district, environment, countArtists, experience, replaced, content, user rating\n",
    "# label is \"approved\"\n",
    "# select features and label e.g. like this \n",
    "# features = data[[\"type\", \"district\", \"...\"]]\n",
    "# label = data[\"approved\"]\n",
    "# or as in the example: features is called X and label is called y\n",
    "# They did it like that:\n",
    "# X = fruits[[\"mass\", \"width\", \"height\"]]\n",
    "# y = fruits[\"fruit_label\"]\n",
    "# They then split the data like this: \n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) \n",
    "# note that random state is for setting seed and this is necessary because they work for a large audience that wants to reproduce the result. \n",
    "# I may not need this parameter, depending on if I want to get a new result each time or not.\n",
    "# Alright, to reliably assess, you even NEED to have a look at multiple splits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisations\n",
    "# to see range of values, outliers and so on (or actually less in my example, because the values are categorical lol)\n",
    "# try to see if a potential algorithm is likely to be able to classify the data -> a well defined cluster should be visible\n",
    "\n",
    "# good plots to visualise the data: \n",
    "# 1) feature-pair plot\n",
    "# their code:\n",
    "from matplotlib import cm\n",
    "cmap = cm.get_cmap(\"gnuplot\")\n",
    "scatter = pd.scatter_matrix(X_train, c = y_train, marker = \"o\", s = 40, hist_kwds = {\"bins\":15}, figsize = (12, 12), cmap = cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MACHINE LEARNING ALGORITHM LEARNING, OPTIONS, DECISION MAKING AND BRAINSTORM\n",
    "# THIS BRANCH AND FILE IS FOR DATA PREPARATION, WRITE THE ACTUAL ML PART IN IT'S OWN BRANCH AND FILE\n",
    "# THIS CELL IS JUST TO TAKE NOTES AND SO ON \n",
    "\"\"\"\n",
    "ML algorithm options\n",
    "-k-nearest neighbors: instance or memory based learning\n",
    "Would likely work for my data set\n",
    "Likely pretty easy: k=1 (one nearest neighbor)\n",
    "It needs: a distance metric, k (how many nearest neighbors), weighting function on the neighbor points (not all neigh. have same influence), method for aggregating classes of neighbor points (how to combine influence and decide)\n",
    "Distance: scikit uses euclidian distance per default\n",
    "Neighbors: e.g. 5 (odd -> no tie -> no weighting necessary)\n",
    "Weighting: not necessary, when k/2=0.5\n",
    "Aggregation: Majority vote\n",
    "Use-case/code in the video: about knn\n",
    "\n",
    "Assess the performance of the algorithm later on:\n",
    "e.g. compute accuracy of the classifier -> score method\n",
    "Also (if I will actually use knn,) try different values for k and then assess accuracy\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
